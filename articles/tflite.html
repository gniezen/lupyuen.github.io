<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="rustdoc">
    <title>Machine Learning on RISC-V BL602 with TensorFlow Lite</title>

    
    <!-- Begin scripts/articles/*-header.html: Article Header for Custom Markdown files processed by rustdoc, like chip8.md -->
<meta property="og:title" 
    content="Machine Learning on RISC-V BL602 with TensorFlow Lite" 
    data-rh="true">
<meta property="og:description" 
    content="How we run TensorFlow Lite on RISC-V BL602... To create a Glowing LED"
    data-rh="true">
<meta property="og:image" 
    content="https://lupyuen.github.io/images/tflite-title.jpg">
<meta property="og:type" 
    content="article" data-rh="true">
<!-- End scripts/articles/*-header.html -->
<!-- Begin scripts/rustdoc-header.html: Header for Custom Markdown files processed by rustdoc, like chip8.md -->
<link rel="alternate" type="application/rss+xml" title="RSS Feed for lupyuen" href="/rss.xml" />
<link rel="stylesheet" type="text/css" href="../normalize.css">
<link rel="stylesheet" type="text/css" href="../rustdoc.css" id="mainThemeStyle">
<link rel="stylesheet" type="text/css" href="../dark.css">
<link rel="stylesheet" type="text/css" href="../light.css" id="themeStyle">
<link rel="stylesheet" type="text/css" href="../prism.css">
<script src="../storage.js"></script><noscript>
<link rel="stylesheet" href="../noscript.css"></noscript>
<link rel="shortcut icon" href="../favicon.ico">
<style type="text/css">
    #crate-search {
        background-image: url("../down-arrow.svg");
    }
    a {
        color: #77d;
    }
</style>
<!-- End scripts/rustdoc-header.html -->


</head>
<body class="rustdoc">
    <!--[if lte IE 8]>
    <div class="warning">
        This old browser is unsupported and will most likely display funky
        things.
    </div>
    <![endif]-->

        <!-- Begin scripts/rustdoc-before.html: Pre-HTML for Custom Markdown files processed by rustdoc, like chip8.md -->

    <!-- Begin Theme Picker -->
    <div class="theme-picker" style="left: 0"><button id="theme-picker" aria-label="Pick another theme!"><img src="../brush.svg"
        width="18" alt="Pick another theme!"></button>
        <div id="theme-choices"></div>
    </div>
    <script src="../theme.js"></script>
    <script src="../prism.js"></script>
    <!-- Theme Picker -->

    <!-- End scripts/rustdoc-before.html -->
    

    <h1 class="title">Machine Learning on RISC-V BL602 with TensorFlow Lite</h1>
    <nav id="TOC"><ul>
<li><a href="#tensorflow-lite-library">1 TensorFlow Lite Library</a><ul></ul></li>
<li><a href="#tensorflow-lite-firmware">2 TensorFlow Lite Firmware</a><ul>
<li><a href="#build-the-firmware">2.1 Build the Firmware</a><ul></ul></li>
<li><a href="#flash-the-firmware">2.2 Flash the Firmware</a><ul></ul></li>
<li><a href="#run-the-firmware">2.3 Run the Firmware</a><ul></ul></li></ul></li>
<li><a href="#machine-learning-in-action">3 Machine Learning in Action</a><ul>
<li><a href="#load-the-model">3.1 Load the Model</a><ul></ul></li>
<li><a href="#run-an-inference">3.2 Run an Inference</a><ul></ul></li></ul></li>
<li><a href="#how-accurate-is-it">4 How Accurate Is It?</a><ul></ul></li>
<li><a href="#how-it-works">5 How It Works</a><ul></ul></li>
<li><a href="#load-tensorflow-model">6 Load TensorFlow Model</a><ul></ul></li>
<li><a href="#run-tensorflow-inference">7 Run TensorFlow Inference</a><ul></ul></li>
<li><a href="#glow-the-led">8 Glow The LED</a><ul></ul></li>
<li><a href="#glowing-machine-learning-in-action">9 Glowing Machine Learning in Action</a><ul></ul></li>
<li><a href="#train-tensorflow-model">10 Train TensorFlow Model</a><ul></ul></li>
<li><a href="#what-else-can-tensorflow-do">11 What Else Can TensorFlow Do?</a><ul></ul></li>
<li><a href="#whats-next">12 What‚Äôs Next</a><ul></ul></li>
<li><a href="#notes">13 Notes</a><ul></ul></li>
<li><a href="#appendix-porting-tensorflow-to-bl602">14 Appendix: Porting TensorFlow to BL602</a><ul>
<li><a href="#source-folders">14.1 Source Folders</a><ul></ul></li>
<li><a href="#compiler-flags">14.2 Compiler Flags</a><ul></ul></li>
<li><a href="#download-libraries">14.3 Download Libraries</a><ul></ul></li>
<li><a href="#optimise-tensorflow">14.4 Optimise TensorFlow</a><ul></ul></li>
<li><a href="#todo">14.5 TODO</a><ul></ul></li></ul></li></ul></nav><p>üìù <em>22 Jun 2021</em></p>
<p>How a Human teaches a Machine to light up an LED‚Ä¶</p>
<blockquote>
<p><em>Human:</em> Hello Machine, please light up the LED in a fun and interesting way.</p>
</blockquote>
<blockquote>
<p><em>Machine:</em> OK I shall light up the LED: on - off - on -off - on - off‚Ä¶</p>
</blockquote>
<p><img src="https://lupyuen.github.io/images/tflite-chart1.jpg" alt="On - Off - On - Off" /></p>
<blockquote>
<p><em>Human:</em> That‚Äôs not very fun and interesting.</p>
</blockquote>
<blockquote>
<p><em>Machine:</em> OK Hooman‚Ä¶ Define fun and interesting.</p>
</blockquote>
<blockquote>
<p><em>Human:</em> Make the LED glow gently brighter and dimmer, brighter and dimmer, and so on.</p>
</blockquote>
<blockquote>
<p><em>Machine:</em> Like a wavy curve? Please teach me to draw a wavy curve.</p>
</blockquote>
<blockquote>
<p><em>Human:</em> Like this‚Ä¶</p>
</blockquote>
<p><img src="https://lupyuen.github.io/images/tflite-chart2.jpg" alt="Wavy Curve" /></p>
<blockquote>
<p><em>Machine:</em> OK I have been trained. I shall now use my trained model to infer the values of the wavy curve. And light up the LED in a fun and interesting way.</p>
</blockquote>
<ul>
<li><a href="https://youtu.be/EFpYJ3qsmEY"><strong>Watch the Demo Video on YouTube</strong></a></li>
</ul>
<p>This sounds like Science Fiction‚Ä¶ But <strong>this is possible today!</strong></p>
<p>(Except for the polite banter)</p>
<p>Read on to learn how <strong>Machine Learning (TensorFlow Lite)</strong> makes this possible on the <strong>BL602 RISC-V SoC</strong>.</p>
<h1 id="tensorflow-lite-library" class="section-header"><a href="#tensorflow-lite-library">1 TensorFlow Lite Library</a></h1>
<p>Remember in our story‚Ä¶</p>
<ol>
<li>
<p>Our Machine <strong>learns to draw a wavy curve</strong></p>
</li>
<li>
<p>Our Machine <strong>reproduces the wavy curve</strong> (to light up the LED)</p>
</li>
</ol>
<p>To accomplish (1) and (2) on BL602, we shall use an open-source <strong>Machine Learning</strong> library: <a href="https://www.tensorflow.org/lite/microcontrollers"><strong>TensorFlow Lite for Microcontrollers</strong></a></p>
<p><em>What‚Äôs a Tensor?</em></p>
<p>Remember these from our Math Textbook? <strong>Scalar, Vector and Matrix</strong></p>
<p><img src="https://lupyuen.github.io/images/tflite-matrix.png" alt="Scalar, Vector, Matrix" /></p>
<p><a href="https://www.tensorflow.org/guide/tensor">(From TensorFlow Guide)</a></p>
<p>When we extend a Matrix from 2D to 3D, we get a <strong>Tensor With 3 Axes</strong>‚Ä¶</p>
<p><img src="https://lupyuen.github.io/images/tflite-tensor.png" alt="Tensor with 3 and 4 Axes" /></p>
<p>And yes we can have a <strong>Tensor With 4 or More Axes</strong>!</p>
<p><strong>Tensors With Multiple Dimensions</strong> are really useful for crunching the numbers needed for Machine Learning.</p>
<p>That‚Äôs how the TensorFlow library works: <strong>Computing lots of Tensors</strong>.</p>
<p>(Fortunately we won‚Äôt need to compute any Tensors ourselves‚Ä¶ The library does everything for us)</p>
<p><a href="https://www.tensorflow.org/guide/tensor">More about Tensors</a></p>
<p><em>Why is the library named TensorFlow?</em></p>
<p>Because it doesn‚Äôt drip, it flows üòÇ</p>
<p>But seriously‚Ä¶ In Machine Learning we push lots of numbers <strong>(Tensors)</strong> through various math functions over specific paths <strong>(Dataflow Graphs)</strong>.</p>
<p>That‚Äôs why it‚Äôs named <strong>‚ÄúTensorFlow‚Äù</strong></p>
<p>(Yes it sounds like the Neural Network in our brain)</p>
<p><a href="https://en.m.wikipedia.org/wiki/TensorFlow">More about TensorFlow</a></p>
<p><em>What‚Äôs the ‚ÄúLite‚Äù version of TensorFlow?</em></p>
<p>TensorFlow normally runs on powerful servers to perform Machine Learning tasks. (Like Speech Recognition and Image Recognition)</p>
<p>We‚Äôre using <strong>TensorFlow Lite</strong>, which is <strong>optimised for microcontrollers</strong>‚Ä¶</p>
<ol>
<li>
<p>Works on microcontrollers with <strong>limited RAM</strong></p>
<p>(Including Arduino, Arm and ESP32)</p>
</li>
<li>
<p>Uses <strong>Static Memory</strong> instead of Dynamic Memory (Heap)</p>
</li>
<li>
<p>But it only supports <strong>Basic Models</strong> of Machine Learning</p>
</li>
</ol>
<p>Today we shall study the TensorFlow Lite library that has been ported to BL602‚Ä¶</p>
<ul>
<li><a href="https://github.com/lupyuen/tflite-bl602"><strong><code>tflite-bl602</code> TensorFlow Lite Library for BL602</strong></a></li>
</ul>
<h1 id="tensorflow-lite-firmware" class="section-header"><a href="#tensorflow-lite-firmware">2 TensorFlow Lite Firmware</a></h1>
<p>Let‚Äôs build, flash and run the TensorFlow Lite Firmware for BL602‚Ä¶ And watch Machine Learning in action!</p>
<h2 id="build-the-firmware" class="section-header"><a href="#build-the-firmware">2.1 Build the Firmware</a></h2>
<p>Download the Firmware Binary File <strong><code>sdk_app_tflite.bin</code></strong> from‚Ä¶</p>
<ul>
<li><a href="https://github.com/lupyuen/bl_iot_sdk/releases/tag/v10.0.0"><strong>Binary Release of <code>sdk_app_tflite</code></strong></a></li>
</ul>
<p>Alternatively, we may build the Firmware Binary File <code>sdk_app_tflite.bin</code> from the <a href="https://github.com/lupyuen/bl_iot_sdk/tree/tflite/customer_app/sdk_app_tflite">source code</a>‚Ä¶</p>
<pre><code class="language-bash"># Download the tflite branch of lupyuen's bl_iot_sdk
git clone --recursive --branch tflite https://github.com/lupyuen/bl_iot_sdk

# TODO: Change this to the full path of bl_iot_sdk
export BL60X_SDK_PATH=$PWD/bl_iot_sdk
export CONFIG_CHIP_NAME=BL602

# Build the firmware
cd bl_iot_sdk/customer_app/sdk_app_tflite
make

# TODO: Change ~/blflash to the full path of blflash
cp build_out/sdk_app_tflite.bin ~/blflash
</code></pre>
<p><a href="https://lupyuen.github.io/articles/pinecone#building-firmware">More details on building bl_iot_sdk</a></p>
<p>(Remember to use the <strong><code>tflite</code></strong> branch, not the default <strong><code>master</code></strong> branch)</p>
<h2 id="flash-the-firmware" class="section-header"><a href="#flash-the-firmware">2.2 Flash the Firmware</a></h2>
<p>Follow these steps to install <code>blflash</code>‚Ä¶</p>
<ol>
<li>
<p><a href="https://lupyuen.github.io/articles/flash#install-rustup"><strong>‚ÄúInstall rustup‚Äù</strong></a></p>
</li>
<li>
<p><a href="https://lupyuen.github.io/articles/flash#download-and-build-blflash"><strong>‚ÄúDownload and build blflash‚Äù</strong></a></p>
</li>
</ol>
<p>We assume that our Firmware Binary File <code>sdk_app_tflite.bin</code> has been copied to the <code>blflash</code> folder.</p>
<p>Set BL602 to <strong>Flashing Mode</strong> and restart the board‚Ä¶</p>
<p><strong>For PineCone:</strong></p>
<ol>
<li>
<p>Set the <strong>PineCone Jumper (IO 8)</strong> to the <strong><code>H</code> Position</strong> <a href="https://lupyuen.github.io/images/pinecone-jumperh.jpg">(Like this)</a></p>
</li>
<li>
<p>Press the Reset Button</p>
</li>
</ol>
<p><strong>For BL10:</strong></p>
<ol>
<li>
<p>Connect BL10 to the USB port</p>
</li>
<li>
<p>Press and hold the <strong>D8 Button (GPIO 8)</strong></p>
</li>
<li>
<p>Press and release the <strong>EN Button (Reset)</strong></p>
</li>
<li>
<p>Release the D8 Button</p>
</li>
</ol>
<p><strong>For Pinenut and MagicHome BL602:</strong></p>
<ol>
<li>
<p>Disconnect the board from the USB Port</p>
</li>
<li>
<p>Connect <strong>GPIO 8</strong> to <strong>3.3V</strong></p>
</li>
<li>
<p>Reconnect the board to the USB port</p>
</li>
</ol>
<p>Enter these commands to flash <code>sdk_app_tflite.bin</code> to BL602 over UART‚Ä¶</p>
<pre><code class="language-bash"># TODO: Change ~/blflash to the full path of blflash
cd ~/blflash

# For Linux:
sudo cargo run flash sdk_app_tflite.bin \
    --port /dev/ttyUSB0

# For macOS:
cargo run flash sdk_app_tflite.bin \
    --port /dev/tty.usbserial-1420 \
    --initial-baud-rate 230400 \
    --baud-rate 230400

# For Windows: Change COM5 to the BL602 Serial Port
cargo run flash sdk_app_tflite.bin --port COM5
</code></pre>
<p><a href="https://lupyuen.github.io/articles/flash#flash-the-firmware">More details on flashing firmware</a></p>
<h2 id="run-the-firmware" class="section-header"><a href="#run-the-firmware">2.3 Run the Firmware</a></h2>
<p>Set BL602 to <strong>Normal Mode</strong> (Non-Flashing) and restart the board‚Ä¶</p>
<p><strong>For PineCone:</strong></p>
<ol>
<li>
<p>Set the <strong>PineCone Jumper (IO 8)</strong> to the <strong><code>L</code> Position</strong> <a href="https://lupyuen.github.io/images/pinecone-jumperl.jpg">(Like this)</a></p>
</li>
<li>
<p>Press the Reset Button</p>
</li>
</ol>
<p><strong>For BL10:</strong></p>
<ol>
<li>Press and release the <strong>EN Button (Reset)</strong></li>
</ol>
<p><strong>For Pinenut and MagicHome BL602:</strong></p>
<ol>
<li>
<p>Disconnect the board from the USB Port</p>
</li>
<li>
<p>Connect <strong>GPIO 8</strong> to <strong>GND</strong></p>
</li>
<li>
<p>Reconnect the board to the USB port</p>
</li>
</ol>
<p>After restarting, connect to BL602‚Äôs UART Port at 2 Mbps like so‚Ä¶</p>
<p><strong>For Linux:</strong></p>
<pre><code class="language-bash">sudo screen /dev/ttyUSB0 2000000
</code></pre>
<p><strong>For macOS:</strong> Use CoolTerm (<a href="https://lupyuen.github.io/articles/flash#watch-the-firmware-run">See this</a>)</p>
<p><strong>For Windows:</strong> Use <code>putty</code> (<a href="https://lupyuen.github.io/articles/flash#watch-the-firmware-run">See this</a>)</p>
<p><strong>Alternatively:</strong> Use the Web Serial Terminal (<a href="https://lupyuen.github.io/articles/flash#watch-the-firmware-run">See this</a>)</p>
<p>We‚Äôre ready to enter the Machine Learning Commands into the BL602 Firmware!</p>
<p><a href="https://lupyuen.github.io/articles/flash#watch-the-firmware-run">More details on connecting to BL602</a></p>
<h1 id="machine-learning-in-action" class="section-header"><a href="#machine-learning-in-action">3 Machine Learning in Action</a></h1>
<p>Remember this <strong>wavy curve</strong>?</p>
<p><img src="https://lupyuen.github.io/images/tflite-chart2.jpg" alt="Wavy Curve" /></p>
<p>We wanted to apply <strong>Machine Learning on BL602</strong> to‚Ä¶</p>
<ol>
<li>
<p><strong>Learn</strong> the wavy curve</p>
</li>
<li>
<p><strong>Reproduce</strong> values from the wavy curve</p>
</li>
</ol>
<p>Watch what happens when we enter the <strong>Machine Learning Commands</strong> into the BL602 Firmware.</p>
<h2 id="load-the-model" class="section-header"><a href="#load-the-model">3.1 Load the Model</a></h2>
<p>We enter this command to load BL602‚Äôs ‚Äúbrain‚Äù with knowledge about the wavy curve‚Ä¶</p>
<pre><code class="language-text"># init
</code></pre>
<p>(Wow wouldn‚Äôt it be great if we could do this for our School Tests?)</p>
<p>Technically we call this <strong>‚ÄúLoading The TensorFlow Lite Model‚Äù.</strong></p>
<p>The <strong>TensorFlow Lite Model</strong> works like a ‚Äúbrain dump‚Äù or ‚Äúknowledge snapshot‚Äù that tells BL602 everything about the wavy curve.</p>
<p>(How did we create the model? We‚Äôll learn in a while)</p>
<h2 id="run-an-inference" class="section-header"><a href="#run-an-inference">3.2 Run an Inference</a></h2>
<p>Now that BL602 has loaded the TensorFlow Lite Model (and knows everything about the wavy curve), let‚Äôs test it!</p>
<p>This command asks BL602 to <strong>infer the output value</strong> of the wavy curve, given the <strong>input value <code>0.1</code></strong>‚Ä¶</p>
<pre><code class="language-text"># infer 0.1
0.160969
</code></pre>
<p>BL602 responds with the <strong>inferred output value <code>0.160969</code></strong></p>
<p><img src="https://lupyuen.github.io/images/tflite-chart3.png" alt="Infer Output Value" /></p>
<p>Let‚Äôs test it with two more <strong>input values: <code>0.2</code> and <code>0.3</code></strong>‚Ä¶</p>
<pre><code class="language-text"># infer 0.2
0.262633

# infer 0.3
0.372770
</code></pre>
<p>BL602 responds with the <strong>inferred output values: <code>0.262633</code> and <code>0.372770</code></strong></p>
<p>That‚Äôs how we <strong>load a TensorFlow Lite Model</strong> on BL602‚Ä¶ And <strong>run an inference</strong> with the TensorFlow Lite Model!</p>
<ul>
<li><a href="https://youtu.be/cCzUFIdUfio"><strong>Watch the Demo Video on YouTube</strong></a></li>
</ul>
<p><img src="https://lupyuen.github.io/images/tflite-run.png" alt="Run TensorFlow Firmware" /></p>
<h1 id="how-accurate-is-it" class="section-header"><a href="#how-accurate-is-it">4 How Accurate Is It?</a></h1>
<p><em>The wavy curve looks familiar‚Ä¶?</em></p>
<p><img src="https://lupyuen.github.io/images/tflite-chart2.jpg" alt="Wavy Curve" /></p>
<p>Yes it was the <strong>Sine Function</strong> all along!</p>
<blockquote>
<p><strong><code>y = sin( x )</code></strong></p>
</blockquote>
<p>(Input value <code>x</code> is in radians, not degrees)</p>
<p><em>So we were using a TensorFlow Lite Model for the Sine Function?</em></p>
<p>Right! The <strong>‚Äú<code>init</code>‚Äù</strong> command from the previous chapter loads a TensorFlow Lite Model that‚Äôs <strong>trained with the Sine Function.</strong></p>
<p><em>How accurate are the values inferred by the model?</em></p>
<p>Sadly Machine Learning Models are <strong>rarely 100% accurate.</strong></p>
<p>Here‚Äôs a comparison of the <strong>values inferred by the model (left)</strong> and the <strong>actual values (right)</strong>‚Ä¶</p>
<p><img src="https://lupyuen.github.io/images/tflite-compare.jpg" alt="Compare inferred vs actual values" /></p>
<p><em>But we can train the model to be more accurate right?</em></p>
<p>Training the Machine Learning Model on too much data may cause <strong>Overfitting</strong>‚Ä¶</p>
<p>When we vary the input value slightly, the <strong>output value may fluctuate wildly</strong>.</p>
<p>(We definitely don‚Äôt want our LED to glow erratically!)</p>
<p><a href="https://en.wikipedia.org/wiki/Overfitting">More about Overfitting</a></p>
<p><em>Is the model accurate enough?</em></p>
<p>Depends how we‚Äôll be using the model.</p>
<p>For glowing an LED it‚Äôs probably OK to use a Machine Learning Model that‚Äôs accurate to <a href="https://en.wikipedia.org/wiki/Significant_figures"><strong>1 Significant Digit</strong></a>.</p>
<p>We‚Äôll watch the glowing LED in a while!</p>
<p><a href="https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/hello_world">(The TensorFlow Lite Model came from this sample code)</a></p>
<h1 id="how-it-works" class="section-header"><a href="#how-it-works">5 How It Works</a></h1>
<p>Let‚Äôs study the code inside the TensorFlow Lite Firmware for BL602‚Ä¶ To understand how it <strong>loads the TensorFlow Lite Model and runs inferences.</strong></p>
<p>Here are the <strong>C++ Global Variables</strong> needed for TensorFlow Lite: <a href="https://github.com/lupyuen/bl_iot_sdk/blob/tflite/customer_app/sdk_app_tflite/sdk_app_tflite/main_functions.cc#L28-L39"><code>main_functions.cc</code></a></p>
<pre><code class="language-c">// Globals for TensorFlow Lite
namespace {
  tflite::ErrorReporter* error_reporter = nullptr;
  const tflite::Model* model = nullptr;
  tflite::MicroInterpreter* interpreter = nullptr;
  TfLiteTensor* input = nullptr;
  TfLiteTensor* output = nullptr;

  constexpr int kTensorArenaSize = 2000;
  uint8_t tensor_arena[kTensorArenaSize];
}
</code></pre>
<ul>
<li>
<p><strong><code>error_reporter</code></strong> will be used for <strong>printing error messages</strong> to the console</p>
</li>
<li>
<p><strong><code>model</code></strong> is the <strong>TensorFlow Lite Model</strong> that we shall load into memory</p>
</li>
<li>
<p><strong><code>interpreter</code></strong> provides the interface for <strong>running inferences</strong> with the TensorFlow Lite Model</p>
</li>
<li>
<p><strong><code>input</code></strong> is the Tensor that we shall set to specify the <strong>input values</strong> for running an inference</p>
</li>
<li>
<p><strong><code>output</code></strong> is the Tensor that will contain the <strong>output values</strong> after running an inference</p>
</li>
<li>
<p><strong><code>tensor_arena</code></strong> is the <strong>working memory</strong> that will be used by TensorFlow Lite to compute inferences</p>
</li>
</ul>
<p>Now we study the code that populates the above Global Variables.</p>
<h1 id="load-tensorflow-model" class="section-header"><a href="#load-tensorflow-model">6 Load TensorFlow Model</a></h1>
<p>Here‚Äôs the <strong>‚Äú<code>init</code>‚Äù command</strong> for our BL602 Firmware: <a href="https://github.com/lupyuen/bl_iot_sdk/blob/tflite/customer_app/sdk_app_tflite/sdk_app_tflite/demo.c#L21-L24"><code>demo.c</code></a></p>
<pre><code class="language-c">/// Command to load the TensorFlow Lite Model (Sine Wave)
static void init(char *buf, int len, int argc, char **argv) {
  load_model();
}
</code></pre>
<p>The command calls <strong><code>load_model</code></strong> to load the TensorFlow Lite Model: <a href="https://github.com/lupyuen/bl_iot_sdk/blob/tflite/customer_app/sdk_app_tflite/sdk_app_tflite/main_functions.cc#L41-L84"><code>main_functions.cc</code></a></p>
<pre><code class="language-c">// Load the TensorFlow Lite Model into Static Memory
void load_model() {
  tflite::InitializeTarget();

  // Set up logging. Google style is to avoid globals or statics because of
  // lifetime uncertainty, but since this has a trivial destructor it's okay.
  static tflite::MicroErrorReporter micro_error_reporter;
  error_reporter = &amp;micro_error_reporter;
</code></pre>
<p>Here we <strong>initialise the TensorFlow Lite Library</strong>.</p>
<p>Next we <strong>load the TensorFlow Lite Model</strong>‚Ä¶</p>
<pre><code class="language-c">  // Map the model into a usable data structure. This doesn't involve any
  // copying or parsing, it's a very lightweight operation.
  model = tflite::GetModel(g_model);
  if (model-&gt;version() != TFLITE_SCHEMA_VERSION) {
    TF_LITE_REPORT_ERROR(error_reporter,
      &quot;Model provided is schema version %d not equal &quot;
      &quot;to supported version %d.&quot;,
      model-&gt;version(), TFLITE_SCHEMA_VERSION);
    return;
  }
</code></pre>
<p><strong><code>g_model</code></strong> contains the <strong>TensorFlow Lite Model Data</strong>, as defined in <a href="https://github.com/lupyuen/bl_iot_sdk/blob/tflite/customer_app/sdk_app_tflite/sdk_app_tflite/model.cc#L28-L238"><code>model.cc</code></a></p>
<p>We create the <strong>TensorFlow Lite Interpreter</strong> that will be called to run inferences‚Ä¶</p>
<pre><code class="language-c">  // This pulls in all the operation implementations we need.
  static tflite::AllOpsResolver resolver;

  // Build an interpreter to run the model with.
  static tflite::MicroInterpreter static_interpreter(
      model, resolver, tensor_arena, kTensorArenaSize, error_reporter);
  interpreter = &amp;static_interpreter;
</code></pre>
<p>Then we <strong>allocate the working memory</strong> that will be used by the TensorFlow Lite Library to compute inferences‚Ä¶</p>
<pre><code class="language-c">  // Allocate memory from the tensor_arena for the model's tensors.
  TfLiteStatus allocate_status = interpreter-&gt;AllocateTensors();
  if (allocate_status != kTfLiteOk) {
    TF_LITE_REPORT_ERROR(error_reporter, &quot;AllocateTensors() failed&quot;);
    return;
  }
</code></pre>
<p>Finally we remember the <strong>Input and Output Tensors</strong>‚Ä¶</p>
<pre><code class="language-c">  // Obtain pointers to the model's input and output tensors.
  input = interpreter-&gt;input(0);
  output = interpreter-&gt;output(0);
}
</code></pre>
<p>Which will be used in the next chapter to run inferences.</p>
<h1 id="run-tensorflow-inference" class="section-header"><a href="#run-tensorflow-inference">7 Run TensorFlow Inference</a></h1>
<p>Earlier we entered this command to <strong>run an inference</strong> with the TensorFlow Lite Model‚Ä¶</p>
<pre><code class="language-text"># infer 0.1
0.160969
</code></pre>
<p>Here‚Äôs the <strong>‚Äú<code>infer</code>‚Äù command</strong> in our BL602 Firmware: <a href="https://github.com/lupyuen/bl_iot_sdk/blob/tflite/customer_app/sdk_app_tflite/sdk_app_tflite/demo.c#L26-L37"><code>demo.c</code></a></p>
<pre><code class="language-c">/// Command to infer values with TensorFlow Lite Model (Sine Wave)
static void infer(char *buf, int len, int argc, char **argv) {
  //  Convert the argument to float
  if (argc != 2) { printf(&quot;Usage: infer &lt;float&gt;\r\n&quot;); return; }
  float input = atof(argv[1]);
</code></pre>
<p>To run an inference, the ‚Äú<code>infer</code>‚Äù command accepts <strong>one input value</strong>: a floating-point number.</p>
<p>We pass the floating-point number to the <strong><code>run_inference</code></strong> function‚Ä¶</p>
<pre><code class="language-c">  //  Run the inference
  float result = run_inference(input);

  //  Show the result
  printf(&quot;%f\r\n&quot;, result);
}
</code></pre>
<p>And we <strong>print the result</strong> of the inference. (Another floating-point number)</p>
<p><strong><code>run_inference</code></strong> is defined in <a href="https://github.com/lupyuen/bl_iot_sdk/blob/tflite/customer_app/sdk_app_tflite/sdk_app_tflite/main_functions.cc#L86-L116"><code>main_functions.cc</code></a> ‚Ä¶</p>
<pre><code class="language-c">// Run an inference with the loaded TensorFlow Lite Model.
// Return the output value inferred by the model.
float run_inference(
  float x) {  //  Value to be fed into the model

  // Quantize the input from floating-point to integer
  int8_t x_quantized = x / input-&gt;params.scale 
    + input-&gt;params.zero_point;
</code></pre>
<p>Interesting Fact: Our TensorFlow Lite Model (for Sine Function) actually accepts an <strong>integer input</strong> and produces an <strong>integer output</strong>! (8-bit integers)</p>
<p>(Integer models run more efficiently on microcontrollers)</p>
<p>The code above <strong>converts the floating-point input</strong> to an 8-bit integer.</p>
<p>We pass the <strong>8-bit integer input</strong> to the TensorFlow Lite Model through the <strong>Input Tensor</strong>‚Ä¶</p>
<pre><code class="language-c">  // Place the quantized input in the model's input tensor
  input-&gt;data.int8[0] = x_quantized;
</code></pre>
<p>Then we <strong>call the interpreter to run the inference</strong> on the TensorFlow Lite Model‚Ä¶</p>
<pre><code class="language-c">  // Run inference, and report any error
  TfLiteStatus invoke_status = interpreter-&gt;Invoke();
  if (invoke_status != kTfLiteOk) {
    TF_LITE_REPORT_ERROR(error_reporter, &quot;Invoke failed on x: %f\n&quot;,
      static_cast&lt;double&gt;(x));
    return 0;
  }
</code></pre>
<p>The 8-bit integer result is <strong>returned through the Output Tensor</strong>‚Ä¶</p>
<pre><code class="language-c">  // Obtain the quantized output from model's output tensor
  int8_t y_quantized = output-&gt;data.int8[0];
</code></pre>
<p>We <strong>convert the 8-bit integer result</strong> to floating-point‚Ä¶</p>
<pre><code class="language-c">  // Dequantize the output from integer to floating-point
  float y = (y_quantized - output-&gt;params.zero_point) 
    * output-&gt;params.scale;

  // Output the results
  return y;
}
</code></pre>
<p>Finally we <strong>return the floating-point result</strong>.</p>
<p>The code we‚Äôve seen is derived from the <a href="https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/hello_world">TensorFlow Lite Hello World Sample</a>, which is covered here‚Ä¶</p>
<ul>
<li>
<p><a href="https://www.tensorflow.org/lite/microcontrollers/get_started_low_level">‚ÄúTensorFlow Lite: Get started with microcontrollers‚Äù</a></p>
</li>
<li>
<p><a href="https://www.tensorflow.org/lite/microcontrollers/library">‚ÄúTensorFlow Lite: Understand the C++ library‚Äù</a></p>
</li>
</ul>
<h1 id="glow-the-led" class="section-header"><a href="#glow-the-led">8 Glow The LED</a></h1>
<p>As promised, now we <strong>light up the BL602 LED with TensorFlow Lite</strong>!</p>
<p>Here‚Äôs the <strong>‚Äú<code>glow</code>‚Äù</strong> command in our BL602 Firmware: <a href="https://github.com/lupyuen/bl_iot_sdk/blob/tflite/customer_app/sdk_app_tflite/sdk_app_tflite/demo.c#L39-L96"><code>demo.c</code></a></p>
<pre><code class="language-c">/// PineCone Blue LED is connected on BL602 GPIO 11
/// TODO: Change the LED GPIO Pin Number for your BL602 board
#define LED_GPIO 11

/// Use PWM Channel 1 to control the LED GPIO.
/// TODO: Select the PWM Channel that matches the LED GPIO
#define PWM_CHANNEL 1

/// Command to glow the LED with values generated by the TensorFlow Lite Model (Sine Wave).
/// We vary the LED brightness with Pulse Widge Modulation:
/// blinking the LED very rapidly with various Duty Cycle settings.
/// See https://lupyuen.github.io/articles/led#from-gpio-to-pulse-width-modulation-pwm
static void glow(char *buf, int len, int argc, char **argv) {
  //  Configure the LED GPIO for PWM
  int rc = bl_pwm_init(
    PWM_CHANNEL,  //  PWM Channel (1) 
    LED_GPIO,     //  GPIO Pin Number (11)
    2000          //  PWM Frequency (2,000 Hz)
  );
  assert(rc == 0);
</code></pre>
<p>The ‚Äú<code>glow</code>‚Äù command takes the <strong>Output Values</strong> from the TensorFlow Lite Model (Sine Function) and sets the <strong>brightness of the BL602 LED</strong>‚Ä¶</p>
<p><img src="https://lupyuen.github.io/images/tflite-chart2.jpg" alt="Wavy Curve" /></p>
<p>The code above configures the <strong>LED GPIO Pin for PWM Output</strong> at 2,000 cycles per second, by calling the <a href="https://lupyuen.github.io/articles/led#how-it-works-bl602-pwm"><strong>BL602 PWM Hardware Abstraction Layer (HAL)</strong></a>.</p>
<p>(PWM or <strong>Pulse Width Modulation</strong> means that we‚Äôll be pulsing the LED very rapidly at 2,000 times a second, to vary the perceived brightness. <a href="https://lupyuen.github.io/articles/led#from-gpio-to-pulse-width-modulation-pwm">See this</a>)</p>
<p>To set the (perceived) LED Brightness, we set the <strong>PWM Duty Cycle</strong> by calling the BL602 PWM HAL‚Ä¶</p>
<pre><code class="language-c">  //  Dim the LED by setting the Duty Cycle to 100%
  rc = bl_pwm_set_duty(
    PWM_CHANNEL,  //  PWM Channel (1) 
    100           //  Duty Cycle (100%)
  );
  assert(rc == 0);
</code></pre>
<p>Here we set the <strong>Duty Cycle to 100%</strong>, which means that the LED GPIO will be <strong>set to High for 100%</strong> of every PWM Cycle.</p>
<p>Our LED <strong>switches off when the LED GPIO is set to High</strong>. Thus the above code effectively sets the <strong>LED Brightness to 0%</strong>.</p>
<p>But PWM won‚Äôt actually start until we do this‚Ä¶</p>
<pre><code class="language-c">  //  Start the PWM, which will blink the LED very rapidly (2,000 times a second)
  rc = bl_pwm_start(PWM_CHANNEL);
  assert(rc == 0);
</code></pre>
<p>Now that <strong>PWM is started</strong> for our LED GPIO, let‚Äôs vary the LED Brightness‚Ä¶</p>
<ol>
<li>
<p>We do this <strong>4 times</strong></p>
<p>(Giving the glowing LED more time to mesmerise us)</p>
</li>
<li>
<p>We step through the <strong>Input Values from <code>0</code> to <code>6.283</code></strong> (or <code>Pi * 2</code>) at intervals of <code>0.05</code></p>
<p>(Because the TensorFlow Lite Model has been trained on Input Values <code>0</code> to <code>Pi * 2</code>‚Ä¶ One cycle of the Sine Wave)</p>
</li>
</ol>
<pre><code class="language-c">  //  Repeat 4 times...
  for (int i = 0; i &lt; 4; i++) {

    //  With input values from 0 to 2 * Pi (stepping by 0.05)...
    for (float input = 0; input &lt; kXrange; input += 0.05) {  //  kXrange is 2 * Pi: 6.283
</code></pre>
<p>Inside the loops, we <strong>run the TensorFlow Lite inference</strong> with the Input Value (<code>0</code> to <code>6.283</code>)‚Ä¶</p>
<pre><code class="language-c">      //  Infer the output value with the TensorFlow Model (Sine Wave)
      float output = run_inference(input);
</code></pre>
<p>(We‚Äôve seen <code>run_inference</code> in the previous chapter)</p>
<p>The TensorFlow Lite Model (Sine Function) produces an <strong>Output Value that ranges from <code>-1</code> to <code>1</code>.</strong></p>
<p>Negative values are not meaningful for setting the LED Brightness, hence we <strong>multiply the Output Value by itself</strong>‚Ä¶</p>
<pre><code class="language-c">      //  Output value has range -1 to 1.
      //  We square the output value to produce range 0 to 1.
      float output_squared = output * output;
</code></pre>
<p>(Why compute <strong>Output Squared</strong> instead of Output Absolute? Because Sine Squared produces a <strong>smooth curve</strong>, whereas Sine Absolute creates a sharp beak)</p>
<p>Next we set the <strong>Duty Cycle to the Output Value Squared</strong>, scaled to 100%‚Ä¶</p>
<pre><code class="language-c">      //  Set the brightness (Duty Cycle) of the PWM LED to the 
      //  output value squared, scaled to 100%
      rc = bl_pwm_set_duty(
        PWM_CHANNEL,                //  PWM Channel (1) 
        (1 - output_squared) * 100  //  Duty Cycle (0% to 100%)
      );
      assert(rc == 0);
</code></pre>
<p>We <strong>flip the LED Brightness</strong> (1 - Output Squared) because‚Ä¶</p>
<ul>
<li>
<p>Duty Cycle = <strong>0%</strong> means <strong>100%</strong> brightness</p>
</li>
<li>
<p>Duty Cycle = <strong>100%</strong> means <strong>0%</strong> brightness</p>
</li>
</ul>
<p>After setting the LED Brightness, we <strong>sleep for 100 milliseconds</strong>‚Ä¶</p>
<pre><code class="language-c">      //  Sleep 100 milliseconds
      time_delay(                //  Sleep by number of ticks (from NimBLE Porting Layer)
        time_ms_to_ticks32(100)  //  Convert 100 milliseconds to ticks (from NimBLE Porting Layer)
      );
    }
  }
</code></pre>
<p><a href="https://lupyuen.github.io/articles/lora2#multitask-with-nimble-porting-layer">(More about NimBLE Porting Layer)</a></p>
<p>And we repeat both loops.</p>
<p>At the end of the command, we <strong>turn off the PWM</strong> for LED GPIO‚Ä¶</p>
<pre><code class="language-c">  //  Stop the PWM, which will stop blinking the LED
  rc = bl_pwm_stop(PWM_CHANNEL);
  assert(rc == 0);
}
</code></pre>
<p>Let‚Äôs run this!</p>
<p><img src="https://lupyuen.github.io/images/tflite-glow.png" alt="Glowing the LED with TensorFlow Lite" /></p>
<h1 id="glowing-machine-learning-in-action" class="section-header"><a href="#glowing-machine-learning-in-action">9 Glowing Machine Learning in Action</a></h1>
<ol>
<li>
<p>Start the <strong>BL602 Firmware for TensorFlow Lite <code>sdk_app_tflite</code></strong></p>
<p>(As described earlier)</p>
</li>
<li>
<p>Enter this command to <strong>load the TensorFlow Lite Model</strong>‚Ä¶</p>
<pre><code class="language-text"># init
</code></pre>
<p>(We‚Äôve seen the ‚Äú<code>init</code>‚Äù command earlier)</p>
</li>
<li>
<p>Then enter this command to <strong>glow the LED with the TensorFlow Lite Model</strong>‚Ä¶</p>
<pre><code class="language-text"># glow
</code></pre>
<p>(Yep the ‚Äú<code>glow</code>‚Äù command from the previous chapter)</p>
</li>
<li>
<p>And the BL602 LED glows gently! Brighter and dimmer, brighter and dimmer, ‚Ä¶</p>
<p>(Though the LED flips on abruptly at the end, because we turned off the PWM)</p>
</li>
</ol>
<ul>
<li><a href="https://youtu.be/EFpYJ3qsmEY"><strong>Watch the Demo Video on YouTube</strong></a></li>
</ul>
<p><img src="https://lupyuen.github.io/images/tflite-chart2.jpg" alt="Wavy Curve" /></p>
<p>(Tip: The <strong>Sine Function</strong> is a terrific way to do things <strong>smoothly and continuously</strong>! Because the derivative of <code>sin(x)</code> is <code>cos(x)</code>, another smooth curve! And the derivative of <code>cos(x)</code> is <code>-sin(x)</code>‚Ä¶ Wow!)</p>
<h1 id="train-tensorflow-model" class="section-header"><a href="#train-tensorflow-model">10 Train TensorFlow Model</a></h1>
<p><img src="https://lupyuen.github.io/images/tflite-meme.jpg" alt="Creating a TensorFlow Lite Model won‚Äôt be easy" /></p>
<p>Sorry Padme, it won‚Äôt be easy to <strong>create and train</strong> a TensorFlow Lite Model.</p>
<p>But let‚Äôs quickly run through the steps‚Ä¶</p>
<p><em>Where is the TensorFlow Lite Model defined?</em></p>
<p><strong><code>g_model</code></strong> contains the <strong>TensorFlow Lite Model Data</strong>, as defined in <a href="https://github.com/lupyuen/bl_iot_sdk/blob/tflite/customer_app/sdk_app_tflite/sdk_app_tflite/model.cc#L28-L238"><code>model.cc</code></a> ‚Ä¶</p>
<pre><code class="language-c">// Automatically created from a TensorFlow Lite flatbuffer using the command:
//   xxd -i model.tflite &gt; model.cc
// This is a standard TensorFlow Lite model file that has been converted into a
// C data array, so it can be easily compiled into a binary for devices that
// don't have a file system.
alignas(8) const unsigned char g_model[] = {
  0x1c, 0x00, 0x00, 0x00, 0x54, 0x46, 0x4c, 0x33, 0x14, 0x00, 0x20, 0x00,
  0x1c, 0x00, 0x18, 0x00, 0x14, 0x00, 0x10, 0x00, 0x0c, 0x00, 0x00, 0x00,
  ...
  0x00, 0x00, 0x00, 0x09};
const int g_model_len = 2488;
</code></pre>
<p>The TensorFlow Lite Model (2,488 bytes) is stored in BL602‚Äôs <strong>XIP Flash ROM</strong>.</p>
<p>This gives the TensorFlow Lite Library more <strong>RAM to run Tensor Computations</strong> for inferencing.</p>
<p>(Remember <strong><code>tensor_arena</code></strong>?)</p>
<p><em>Can we create and train this model on BL602?</em></p>
<p>Sorry Padme nope.</p>
<p>Training a TensorFlow Lite Model requires <strong>Python</strong>. Thus we need a Linux, macOS or Windows computer.</p>
<p>Here‚Äôs the <strong>Python Jupyter Notebook</strong> for training the TensorFlow Lite Model that we have used‚Ä¶</p>
<ul>
<li><a href="https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb?authuser=0">‚ÄúHello World: Jupyter Notebook on Google Colaboratory‚Äù</a></li>
</ul>
<p>Check out the docs on <strong>training and converting TensorFlow Lite Models</strong>‚Ä¶</p>
<ul>
<li>
<p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/train/README.md">‚ÄúTensorFlow Lite: Hello World Training‚Äù</a></p>
</li>
<li>
<p><a href="https://www.tensorflow.org/lite/microcontrollers/build_convert">‚ÄúTensorFlow Lite: Build and convert models‚Äù</a></p>
</li>
</ul>
<h1 id="what-else-can-tensorflow-do" class="section-header"><a href="#what-else-can-tensorflow-do">11 What Else Can TensorFlow Do?</a></h1>
<p>Even though we‚Äôve used TensorFlow Lite for a trivial task (glowing an LED)‚Ä¶ There are <strong>so many possibilities</strong>!</p>
<ol>
<li>
<p>PineCone BL602 Board has a <strong>3-in-1 LED: Red + Green + Blue</strong>.</p>
<p>We could control all 3 LEDs and glow them in a dazzling, multicolour way!</p>
<p>(The TensorFlow Lite Model would probably produce an Output Tensor that contains 3 Output Values)</p>
</li>
<li>
<p>Light up an LED when it <strong>detects my face</strong></p>
<p>TODO</p>
<p><a href="https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/person_detection">Check out this sample code</a></p>
</li>
<li>
<p>Speech</p>
<p>TODO</p>
<p><a href="https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/micro_speech">Check out this sample code</a></p>
</li>
<li>
<p>Motion Gestures (via Accelerometer)</p>
<p>TDOD</p>
<p><a href="https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/magic_wand">Check out this sample code</a></p>
</li>
</ol>
<h1 id="whats-next" class="section-header"><a href="#whats-next">12 What‚Äôs Next</a></h1>
<p>TODO</p>
<ul>
<li>
<p><a href="https://github.com/sponsors/lupyuen">Sponsor me a coffee</a></p>
</li>
<li>
<p><a href="https://lupyuen.github.io/articles/book">Read ‚ÄúThe RISC-V BL602 Book‚Äù</a></p>
</li>
<li>
<p><a href="https://lupyuen.github.io">Check out my articles</a></p>
</li>
<li>
<p><a href="https://lupyuen.github.io/rss.xml">RSS Feed</a></p>
</li>
</ul>
<p><em>Got a question, comment or suggestion? Create an Issue or submit a Pull Request here‚Ä¶</em></p>
<p><a href="https://github.com/lupyuen/lupyuen.github.io/blob/master/src/tflite.md"><code>lupyuen.github.io/src/tflite.md</code></a></p>
<h1 id="notes" class="section-header"><a href="#notes">13 Notes</a></h1>
<ol>
<li>This article is the expanded version of <a href="https://twitter.com/MisterTechBlog/status/1402531760764641280">this Twitter Thread</a></li>
</ol>
<h1 id="appendix-porting-tensorflow-to-bl602" class="section-header"><a href="#appendix-porting-tensorflow-to-bl602">14 Appendix: Porting TensorFlow to BL602</a></h1>
<p>TODO</p>
<h2 id="source-folders" class="section-header"><a href="#source-folders">14.1 Source Folders</a></h2>
<p>TODO</p>
<p><img src="https://lupyuen.github.io/images/tflite-source.png" alt="Source Folders" /></p>
<p>TODO17</p>
<h2 id="compiler-flags" class="section-header"><a href="#compiler-flags">14.2 Compiler Flags</a></h2>
<p>TODO</p>
<p><img src="https://lupyuen.github.io/images/tflite-cppflags.png" alt="Compiler Flags" /></p>
<p>TODO5</p>
<p><img src="https://lupyuen.github.io/images/tflite-cppflags2.png" alt="Compiler Flags" /></p>
<p>TODO6</p>
<h2 id="download-libraries" class="section-header"><a href="#download-libraries">14.3 Download Libraries</a></h2>
<p>TODO</p>
<p><img src="https://lupyuen.github.io/images/tflite-gemmlowp.png" alt="Download gemmlowp" /></p>
<p>TODO8</p>
<p><img src="https://lupyuen.github.io/images/tflite-gemmlowp2.png" alt="Download gemmlowp" /></p>
<p>TODO9</p>
<p><img src="https://lupyuen.github.io/images/tflite-ruy.png" alt="Download ruy" /></p>
<p>TODO14</p>
<h2 id="optimise-tensorflow" class="section-header"><a href="#optimise-tensorflow">14.4 Optimise TensorFlow</a></h2>
<p>TODO</p>
<ul>
<li><a href="https://www.tensorflow.org/lite/microcontrollers/library#optimized_kernels">‚ÄúTensorFlow Lite: Optimised Kernels‚Äù</a></li>
</ul>
<h2 id="todo" class="section-header"><a href="#todo">14.5 TODO</a></h2>
<p><img src="https://lupyuen.github.io/images/tflite-cmath.png" alt="" /></p>
<p>TODO3</p>
<p><img src="https://lupyuen.github.io/images/tflite-commands.png" alt="" /></p>
<p>TODO4</p>
<p><img src="https://lupyuen.github.io/images/tflite-dsohandle.png" alt="" /></p>
<p>TODO7</p>
<p><img src="https://lupyuen.github.io/images/tflite-infer.png" alt="" /></p>
<p>TODO10</p>
<p><img src="https://lupyuen.github.io/images/tflite-initstatic.png" alt="" /></p>
<p>TODO11</p>
<p><img src="https://lupyuen.github.io/images/tflite-math.png" alt="" /></p>
<p>TODO12</p>
<p><img src="https://lupyuen.github.io/images/tflite-flatbuffers.png" alt="" /></p>
<p>TODO13</p>
<p><img src="https://lupyuen.github.io/images/tflite-setup.png" alt="" /></p>
<p>TODO15</p>
<p><img src="https://lupyuen.github.io/images/tflite-loop.png" alt="" /></p>
<p>TODO16</p>
<p><img src="https://lupyuen.github.io/images/tflite-static.png" alt="" /></p>
<p>TODO18</p>
<p><img src="https://lupyuen.github.io/images/tflite-undefined.png" alt="" /></p>
<p>TODO19</p>
<p><img src="https://lupyuen.github.io/images/tflite-undefined2.png" alt="" /></p>
<p>TODO20</p>
<p><img src="https://lupyuen.github.io/images/tflite-undefined3.png" alt="" /></p>
<p>TODO21</p>
<p><img src="https://lupyuen.github.io/images/tflite-undefined4.png" alt="" /></p>
<p>TODO22</p>
<p><img src="https://lupyuen.github.io/images/tflite-build.png" alt="" /></p>
<p>TODO2</p>

    
</body>
</html>